## Activation functions and their performance conditioned on dimensions of data

As part of the class project for Machine Learning II @Frankfurt School, we dive deeper into undertanding Activation Functions (AFs) used in Neural Networks.

In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. We study 7 different AFs in total, a few convetional and the others state-of-the-art.

The performance of neural network models with different activation functions, keeping all other relevant variables constant, is studied for the following 2 datasets:
1) Magic Gamma Telescope Dataset - (19020,11); 2 classes; https://archive.ics.uci.edu/ml/datasets/magic+gamma+telescope
2) Covertype Dataset - (581012,54); 7 classes; https://archive.ics.uci.edu/ml/datasets/Covertype

Structure of the repository:-
- 1_Modelling: Contains python files with models using different AFs and datasets.
- 2_Visualisation: Contains convergence visualisations for 'loss' and 'accuracy' for different runs.

Contributors:-
Jerome (@silvertuna); Rhushikesh (@rhushikezh); Neelesh (@neelblabla); Nils (@nilsmart96); Kirtesh (@kirteshpatel98)

 
